@inproceedings{Silver:2014:DPG:3044805.3044850,
	author = {Silver, David and Lever, Guy and Heess, Nicolas and Degris, Thomas and Wierstra, Daan and Riedmiller, Martin},
	title = {Deterministic Policy Gradient Algorithms},
	booktitle = {Proceedings of the 31st International Conference on International Conference on Machine Learning - Volume 32},
	series = {ICML'14},
	year = {2014},
	location = {Beijing, China},
	pages = {I-387--I-395},
	url = {http://dl.acm.org/citation.cfm?id=3044805.3044850},
	acmid = {3044850},
	publisher = {JMLR.org},
} 

@article{DBLP:journals/corr/abs-1812-05905,
	author    = {Tuomas Haarnoja and
	Aurick Zhou and
	Kristian Hartikainen and
	George Tucker and
	Sehoon Ha and
	Jie Tan and
	Vikash Kumar and
	Henry Zhu and
	Abhishek Gupta and
	Pieter Abbeel and
	Sergey Levine},
	title     = {Soft Actor-Critic Algorithms and Applications},
	journal   = {CoRR},
	volume    = {abs/1812.05905},
	year      = {2018},
	url       = {http://arxiv.org/abs/1812.05905},
	archivePrefix = {arXiv},
	eprint    = {1812.05905},
	timestamp = {Tue, 01 Jan 2019 15:01:25 +0100},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1812-05905},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/SchulmanWDRK17,
	author    = {John Schulman and
	Filip Wolski and
	Prafulla Dhariwal and
	Alec Radford and
	Oleg Klimov},
	title     = {Proximal Policy Optimization Algorithms},
	journal   = {CoRR},
	volume    = {abs/1707.06347},
	year      = {2017},
	url       = {http://arxiv.org/abs/1707.06347},
	archivePrefix = {arXiv},
	eprint    = {1707.06347},
	timestamp = {Mon, 13 Aug 2018 16:47:34 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/SchulmanWDRK17},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1801-01290,
	author    = {Tuomas Haarnoja and
	Aurick Zhou and
	Pieter Abbeel and
	Sergey Levine},
	title     = {Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning
	with a Stochastic Actor},
	journal   = {CoRR},
	volume    = {abs/1801.01290},
	year      = {2018},
	url       = {http://arxiv.org/abs/1801.01290},
	archivePrefix = {arXiv},
	eprint    = {1801.01290},
	timestamp = {Mon, 13 Aug 2018 16:48:10 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1801-01290},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1803-07067,
	author    = {A. Rupam Mahmood and
	Dmytro Korenkevych and
	Brent J. Komer and
	James Bergstra},
	title     = {Setting up a Reinforcement Learning Task with a Real-World Robot},
	journal   = {CoRR},
	volume    = {abs/1803.07067},
	year      = {2018},
	url       = {http://arxiv.org/abs/1803.07067},
	archivePrefix = {arXiv},
	eprint    = {1803.07067},
	timestamp = {Mon, 13 Aug 2018 16:46:54 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1803-07067},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/DuanCHSA16,
	author    = {Yan Duan and
	Xi Chen and
	Rein Houthooft and
	John Schulman and
	Pieter Abbeel},
	title     = {Benchmarking Deep Reinforcement Learning for Continuous Control},
	journal   = {CoRR},
	volume    = {abs/1604.06778},
	year      = {2016},
	url       = {http://arxiv.org/abs/1604.06778},
	archivePrefix = {arXiv},
	eprint    = {1604.06778},
	timestamp = {Mon, 03 Sep 2018 12:15:29 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/DuanCHSA16},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/abs-1802-01561,
	author    = {Lasse Espeholt and
	Hubert Soyer and
	R{\'{e}}mi Munos and
	Karen Simonyan and
	Volodymyr Mnih and
	Tom Ward and
	Yotam Doron and
	Vlad Firoiu and
	Tim Harley and
	Iain Dunning and
	Shane Legg and
	Koray Kavukcuoglu},
	title     = {{IMPALA:} Scalable Distributed Deep-RL with Importance Weighted Actor-Learner
	Architectures},
	journal   = {CoRR},
	volume    = {abs/1802.01561},
	year      = {2018},
	url       = {http://arxiv.org/abs/1802.01561},
	archivePrefix = {arXiv},
	eprint    = {1802.01561},
	timestamp = {Mon, 13 Aug 2018 01:00:00 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1802-01561},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/nature/MnihKSRVBGRFOPB15,
	author    = {Volodymyr Mnih and
	Koray Kavukcuoglu and
	David Silver and
	Andrei A. Rusu and
	Joel Veness and
	Marc G. Bellemare and
	Alex Graves and
	Martin A. Riedmiller and
	Andreas Fidjeland and
	Georg Ostrovski and
	Stig Petersen and
	Charles Beattie and
	Amir Sadik and
	Ioannis Antonoglou and
	Helen King and
	Dharshan Kumaran and
	Daan Wierstra and
	Shane Legg and
	Demis Hassabis},
	title     = {Human-level control through deep reinforcement learning},
	journal   = {Nature},
	volume    = {518},
	number    = {7540},
	pages     = {529--533},
	year      = {2015},
	url       = {https://doi.org/10.1038/nature14236},
	doi       = {10.1038/nature14236},
	timestamp = {Wed, 14 Nov 2018 00:00:00 +0100},
	biburl    = {https://dblp.org/rec/bib/journals/nature/MnihKSRVBGRFOPB15},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/WangFL15,
	author    = {Ziyu Wang and
	Nando de Freitas and
	Marc Lanctot},
	title     = {Dueling Network Architectures for Deep Reinforcement Learning},
	journal   = {CoRR},
	volume    = {abs/1511.06581},
	year      = {2015},
	url       = {http://arxiv.org/abs/1511.06581},
	archivePrefix = {arXiv},
	eprint    = {1511.06581},
	timestamp = {Mon, 13 Aug 2018 16:48:17 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/WangFL15},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{DBLP:journals/corr/HasseltGS15,
	author    = {Hado van Hasselt and
	Arthur Guez and
	David Silver},
	title     = {Deep Reinforcement Learning with Double Q-learning},
	journal   = {CoRR},
	volume    = {abs/1509.06461},
	year      = {2015},
	url       = {http://arxiv.org/abs/1509.06461},
	archivePrefix = {arXiv},
	eprint    = {1509.06461},
	timestamp = {Mon, 13 Aug 2018 16:47:32 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/HasseltGS15},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{DBLP:books/lib/SuttonB98,
	author    = {Richard S. Sutton and
	Andrew G. Barto},
	title     = {Reinforcement learning - an introduction},
	series    = {Adaptive computation and machine learning},
	publisher = {{MIT} Press},
	year      = {1998},
	url       = {http://www.worldcat.org/oclc/37293240},
	isbn      = {0262193981},
	timestamp = {Wed, 26 Apr 2017 17:48:08 +0200},
	biburl    = {https://dblp.org/rec/bib/books/lib/SuttonB98},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{DBLP:books/lib/WatkinsC1989,
	author    = {Watkins, C. J. C. H. },
	title     = {Learning from Delayed Rewards},
	publisher = {University of Cambridge},
	year      = {1989}
}

@article{DBLP:journals/corr/SchaulQAS15,
	author    = {Tom Schaul and
	John Quan and
	Ioannis Antonoglou and
	David Silver},
	title     = {Prioritized Experience Replay},
	journal   = {CoRR},
	volume    = {abs/1511.05952},
	year      = {2015},
	url       = {http://arxiv.org/abs/1511.05952},
	archivePrefix = {arXiv},
	eprint    = {1511.05952},
	timestamp = {Mon, 13 Aug 2018 16:46:28 +0200},
	biburl    = {https://dblp.org/rec/bib/journals/corr/SchaulQAS15},
	bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{pmlr-v32-thomas14,
	title = 	 {Bias in Natural Actor-Critic Algorithms},
	author = 	 {Philip Thomas},
	booktitle = 	 {Proceedings of the 31st International Conference on Machine Learning},
	pages = 	 {441--448},
	year = 	 {2014},
	editor = 	 {Eric P. Xing and Tony Jebara},
	volume = 	 {32},
	number =       {1},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Bejing, China},
	month = 	 {22--24 Jun},
	publisher = 	 {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v32/thomas14.pdf},
	url = 	 {http://proceedings.mlr.press/v32/thomas14.html},
	abstract = 	 {We show that several popular discounted reward natural actor-critics, including the popular NAC-LSTD and eNAC algorithms, do not generate unbiased estimates of the natural policy gradient as claimed. We derive the first unbiased discounted reward natural actor-critics using batch and iterative approaches to gradient estimation. We argue that the bias makes the existing algorithms more appropriate for the average reward setting. We also show that, when Sarsa(lambda) is guaranteed to converge to an optimal policy, the objective function used by natural actor-critics is concave, so policy gradient methods are guaranteed to converge to globally optimal policies as well.}
}